<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Author Identification using different classification methods</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/round-about.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#">Cloud Project Group 9</a> 
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                  
                    <li>
                        <a href="index.html">Overview</a>
                    </li>
                    <li>
                        <a href="motive.html">Motivation</a>
                    </li>
                    <li>
                        <a href="implem.html">Implementation</a>
                    </li>
                     <li>
                        <a href="data.html">Dataset</a>
                    </li>
                    <li>
                        <a href="perform.html">Performance</a>
                    </li>
                    <li>
                        <a href="results.html">Results</a>
                    </li>
                     <li>
                        <a href="accomp.html">Accomplishments</a>
                    </li>
                    <li>
                        <a href="roles.html">Roles</a>
                    </li>
                     <li>
                        <a href="ref.html">Observations and External References</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Content -->
    <div class="container">

        <!-- Introduction Row -->
        <div class="row">
            <div class="col-lg-12">
                <h1 class="page-header">
                    <small>Author Identification using different classification methods</small>
                </h1>
                <p>
                    CS Graduate Students at University of North Carolina Charlotte
                </p>
            </div>
        </div>

        <!-- Team Members Row -->
          <div class="row">
            <div class="col-lg-12">
                <h2 class="page-header">Our Team</h2>
            </div>
            <h3>Allen Sylvester Irudayaraj</h3>
            <h3>Advaith Auron Suresh</h3>
            <h3>VenkateshPerumal Chockalingam</h3>
           
        </div>
        <div class="container">
        <h1>Logistic Regression</h1>
        <p>
        
        
            <b></b>
            <ul>
            <li> We have implemented a standard logistic regression model in this project. We aim at obtaining weights which could maximize the likeliness or likelihood of obtaining our data and model it in such a way that it is able to clearly distinguish the target values. We use the concept of maximum likelihood estimation to obtain the result. The problem of optimization is resolved using gradient ascent as the maximization of likelihood in logistic regression is devoid of a closed form solution.  
            </li>
            <b>Processing Steps</b>
            <li>
            We would be providing classification results for each author and approach the problem as a 2-class classifier and not a k-class classifier. The result would hence be of the form: Document belongs to the author or Document does not belong to the author. We then bifurcate the data into a training set (70%) and a test set (30%). We train the logistic regression model on the training set and predict the outcome based on the test set. 
            </li>
            <b>Data Preparation</b>
            <li>
           The data obtained from the dataset is processed before feeding it to the model. We use RDDs to separate files belonging to one class from the others. We then obtain our training data and test data by randomly splitting it. 
First, we obtain each file as a key value pair containing the filename and its contents as a key and value pair. We iterate through the document corpus for our desired class and model a dictionary of the words used in it. We use a CountVectorizer function defined in the sklearn library to achieve this and obtain a term frequency of words used in each document of the entire training corpus. As a result, we obtain a feature matrix containing the frequency count of each word in a document.
We use the CountVectorizer function along with some options that allow us to prepare our data for better modeling. The parameters given were:
            <b>Input = ‘content’:</b>
            the input is expected to be the sequence strings or bytes items are expected to be analyzed directly
            <b>stop_words = ‘english’:</b>
            If ‘english’, a built-in stop word list for English is used
            <b>lowercase = True:</b>
            Convert all characters to lowercase before tokenizing
            <b> max_df = 0.6:</b>
            When building the vocabulary, ignores terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts
            <b> min_df = 5: </b>
            When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.
            <b> max_features = 1000: </b>
            If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus
            <b> Sample Data: </b>
            The following are a few lines from “Anthony Trollope__A Ride Across Palestine”

Circumstances took me to the Holy Land without a companion, and compelled me to visit Bethany, the Mount of Olives, and the Church of the Sepulchre alone.  I acknowledge myself to be a gregarious animal, or, perhaps, rather one of those which nature has intended to go in pairs.

            </li>
            <b>Maximum Likelihood:</b>
            <li>
            To obtain the maximum likelihood, we use log likelihood as log transformations are monotonic and do not affect weight parameters. We define log-likelihood as a cumulative sum of the training data.
It is given by the equation:

where xi is the feature data, β is weight and y is the target

To obtain the gradient of the log-likelihood, we take a derivative of the given equation and represent it in the form of a matrix, that gives us:

            </li>
            </ul>

        
        <h1>Naive Bayes Classifier</h1>
        <b> </b>
        <p><b>Computations followed for implementation</b></p><br>
        <ol>
        <li> Calculation of Prior Probability on Training set </li>
        <p> Consider Author “Charles Dickens”, 
            
            <div class="col-lg-14 col-sm-16 text-center">
                <img class="img-responsive img-center" src="NBC2.png" alt="" >
                <p></p>
            </div>

        Similarly, the prior probability for every other author is calculated. </p>
        <li> Conditional Probability Calculation on training set </li>
        <p> Conditional probability is calculated for all the unique words for each author corpus.
        Consider there is a word “Though” in a document in “Charles Dickens” corpus, then the conditional probability of the word is, 
             <div class="col-lg-14 col-sm-16 text-center">
                <img class="img-responsive img-center" src="NBC3.png" alt="" >
                <p></p>
            </div>
        </p>
        <li> Testing Part Computation </li>
        <p> Consider if a test document “Doc1” is selected, then the probability of that document belonging to an Author (Class) is calculated by the following formula,
             <div class="col-lg-14 col-sm-16 text-center">
                <img class="img-responsive img-center" src="NBC4.png" alt="" >
                <p></p>
            </div>
        </p>
        </ol>
         
        
        <p><b>Steps Followed for implementation</b></p><br>
        <ol>
          <li>The documents are first separated into training set and test set.</li>
          <li>First Prior probability of all the classes (Authors) in the training set is calculated.</li>
          <li>Conditional Probabilities for all the unique words is calculated for each Author corpus.</li>
          <li>Then From test set each Document is selected.</li>
          <li>The probability of that document belonging to a class (Author) is calculated. This probability is calculated for all the classes (Author).</li> 
          <li>Class (Author) with highest probability is selected as the class (Author) to which the selected test document belongs to.</li>
         
        </ol>
        
        <h1>K Nearest Neigbors</h1>
        <p><b>Computations followed for implementation</b></p><br>
        <ol>
        <li> TFIDF Score Computation </li>
        <p> 
        TFIDF = Term frequency (TF) * Inverse Document Frequency (IDF)
        •   TF (t, d) = 1 + log10(Count of word w in document d)
        Where TF is calculated as number of occurrences of a word w in a document d. 
        •   IDF (t) = 1 + log10(N/DF)
        Where N = Total number of documents for a single author
        DF = Number of documents containing the word w for a single author corpus.
        </p>
        <li> Euclidean Distance Computation </li>
        <p>
        Euclidean distance determines the vector distance between the documents by using the following equation,
        D (x, y) = √∑Nr=1 (arx – ary)2
        Where D (x, y) is the distance between two documents x and y.
        N is the number of unique words in both the document x and y.
            arx is the TFIDF score of the word r in document x (Training Document).
            ary is the TFIDF score of the word r in document y (Test Document).
        </p>
        </ol>
        <li> <b></b>        
            <p><b>Steps Followed for implementation</b></p><br>
          <ul><ol> <li>TFIDF scores of all unique words is computed for each Author in training corpus. </li> 
<li>TFIDF scores of all unique words is computed for each author in test corpus.</li>
<li>Euclidean distance is calculated by comparing the selected test document TFIDF scores with TFIDF scores of each document present in the training set.</li>
<li>The documents are ranked in ascending order.</li>
<li>K value is selected, and first K number of documents are selected from the ranked list.</li>
<li>Author with highest number of document in the K list, classified as author for the test document selected.
</li>
            </ol> <br> <br>
            
        </div>

        <hr>

        <!-- Footer -->
        <footer>
           
                <!-- /.col-lg-12 -->
            </div>
            <!-- /.row -->
        </footer>

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
